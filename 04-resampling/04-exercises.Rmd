---
title: "Sensitivity and Specificity"
author: "Jarus Singh"
date: "October 5, 2015"
output: html_document
---


## Readings

***APM***

- ***Chapter 5 Measuring Performance in Regression Models*** (esp. ***5.2 The Variance Bias Trade-Off***)  (5 pages)
- ***Chapter 11 Measuring Performance in Classification Models*** (~20 pages)
- ***Chapter 7.4 K-Nearest Neighbors (regression)*** (2 pages)
- ***Chapter 13.5 K-Nearest Neighbors (classification)*** (3 pages)


```{r, echo=FALSE, results='hide', warning=FALSE }
packs <-  c('ggplot2', 'magrittr', 'dplyr', 'caret', 'AppliedPredictiveModeling')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

.. = NULL  # For Aesthetics

```


## EXERCISE 1: Resampling

`x` is a random variable. We want to not only know what the `mean(x)` is but want to calculate the uncertainty of `mean(x)`.  Measuring the uncertainty requires repeated measurements of `mean(x)`.

- Calculate the mean of `x`.
- Calculte the `sd( mean(x) )` using the **using 10-fold cross-validation**.  Create your own folds, show your work. (An example is for the Bootstrap is given as a hint. )


```{r}
set.seed(1) 
x <- runif(20,1,20)

x_mean = mean(x)

k=10

# CROSS-VALIDATION
# Randomizes the order of x
x <- x[order(rnorm(20))]

# Finds sd_cv of randomly ordered x
sd_cv <- sapply(1:k, function(i) x[-c(2*i-1,2*i)] %>% mean) %>% sd

# BOOTSTRAP (EXAMPLE)
sd_boot <- sapply(1:k, function(i) sample(x,replace=TRUE) %>% mean ) %>% sd

```


- sd_cv   is: `r sd_cv`
- sd_boot is: `r sd_boot`



# Exercise 2: Binomial Metrics

Here's a really simple Model of Versicolor iris based on the **iris** data :

```{r}
set.seed(1)
data(iris)

qplot( data=iris, x=Petal.Length, y=Sepal.Length, color=Species )

# Create Dependent Variable
iris$Versicolor <- 
  ifelse( iris$Species == 'versicolor', "versicolor", "other" ) %>% as.factor
iris$Species = NULL 

wh <- sample.int( nrow(iris), size=nrow(iris)/2 )
train <- iris[ wh,]
test <- iris[ -wh, ]


fit.glm <- glm( Versicolor ~ . - Sepal.Length, data=train, family=binomial )
```


Use the models to and write functions to calculate:

* Prevalence 
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

The functions should take two logical vectors of the same length, `y` and `yhat`

```{r}

prevalence = function(y,yhat) {
  # Takes all positives [sum(y)] over total samples [length(y)]
  sum(y) / length(y)
}
accuracy   =  function(y, yhat) {
  # sum(y == yhat) is TRUE if values match, sum is the total
  # number of classifications that match over the total samples [length(y)]
  sum(y == yhat) / length(y)
}
error_rate = function(y, yhat) {
  # Could also have used 1 - accuracy(y,yhat) but
  # that may have been against the spirit of the assignment
  sum(y != yhat) / length(y)
}
tpr = function(y, yhat) {
  # Numerator: y has to be true, and yhat has to be true
  # to have correctly identified y
  sum(y * yhat) /
  # Denominator: total number of true positive
  sum(y)
}
fpr = function(y,yhat) {
  sum(y & (y != yhat)) /
    sum(!y)
}      # See Example
tnr = function(y, yhat) {
  # y == yhat finds all correct specifications, !y matches them on
  # only true negatives, then divide by all true negatives
  sum(!y & (y == yhat)) /
    sum(!y)
}
# Note: Same as true positive rate
sensitivity = function(y, yhat) {
  # Numerator: y has to be true, and yhat has to be true
  # to have correctly identified y
  sum(y * yhat) /
  # Denominator: total number of true positive
  sum(y)
}
# Note: same as tnr
specificity = function(y, yhat) {
  sum(!y & (y == yhat)) /
    sum(!y)
}
# Note: same as TPR, sensitivity
recall = function(y, yhat) {
  sum(y * yhat) /
    sum(y)
}
precision = function(y, yhat) {
  sum(y * yhat) /
    sum(yhat)
}

# EXAMPLE: fpr
# The FPR is THE NUMBER OF FALSE POSITIVES / NEGATIVES (TN+FP)

threshold = 0.5 
y = test$Versicolor == 'versicolor'
yhat = predict(fit.glm, test, type="response") > threshold


fpr = function(y,yhat)
  sum(y & (y != yhat) ) / # FP
  sum(! y)                # N

fpr(y,yhat)

```

- What is wrong with the modeling approach used?
That's a very good question, I had to think about this for a while and don't have a definitive answer. Here are some criticisms though.

1. The plot above shows that versicolor species show up in an obvious area when charting Sepal.Length vs. Petal.Length. However, when you look at the model specification, Sepal.Length is excluded, but Petal.Width and Petal.Length (which are highly correlated) are both included. This could have resulted in overfitting.
2. The data was split into equal sized training and test sets. Given only 150 total data points, it's possible that one could have constructed a much better model using more data in the training set.
3. Related to 1, given the plot of the data, it looks like a k-nearest neighbors model could perform quite well instead of the logistic regression we do instead.


